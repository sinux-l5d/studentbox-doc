{"/":{"title":"üë®‚Äçüíª Studentbox documentation","content":"\n_Studentbox_ is a project that aims to make deployment easy for student üë®‚Äçüéì, and management of hosted projects simple for a teacher üë®‚Äçüè´.\nThe repository is hosted on [Github](https://github.com/sinux-l5d/studentbox).\n\n# üåç Context\n\nI've realised this project during my BSc (Hons)[^bsc] at [Dundalk Institute of Technology](https://www.dkit.ie/). The curriculum is about _Cloud Computing_, and so this project has to take advantage of different cloud aspects like including scaling, APIs[^api] and automation etc.\n\nSee the [[about-me|About me]] page this know more about the author.\n\n# ü•ú In a nutshell...\n\n_Studentbox_ wants to archive the following goals:\n\n- Host the students' projects in an isolated environment, without impacting other projects or host system\n- Provide a complete environment for the apps supported\n- Be able to deploy in a single command line...\n  - PHP app using MySQL\n  - Node.js app using MongoDB or MySQL\n- The _CLI_[^cli] client should...\n  - work on Linux, Mac and Windows\n  - be easy to setup\n  - be usable by both student and teacher\n- Make possible live-reloading while developing\n\nWhile having the following constraints:\n\n- Hostable on AWS\n- Self-hostable on an on-premise server\n\n# üèÅ Where to start\n\nYou can access the [[project/goals|goals]] page to know more about the project's goals and the [[project/road-map|road map]].\nThere is also the [[tags/journal|journal entries]].\n\n# üìÑ Poster\n\nDuring the project, I had to make a poster to present it. You can find it \u003ca href=\"poster.pdf\"\u003ehere\u003c/a\u003e.\n\n[^bsc]:\n    **B**achelor of **Sc**ience (**Hon**our**s**) is the 4th year after secondary school (in Ireland).\n    In France, the equivalent year is the 1st year of Master's degree (the 4th year after _Lyc√©e_).\n\n[^api]: **A**pplication **P**rogramming **I**nterface in IT is an abstraction layer to interact with things we don't want to deal manually with. In this curriculum, we're talking about Web APIs (REST with JSON among others).\n[^cli]:\n    **C**ommand **L**ine **I**nterface, as the opposit of Graphical User Interface (GUI) is a text-based interface.\n    On Windows, it's in CMD/PowerShell. On Linux, it's in your favourite terminal \u0026 shell (bash, zsh...)\n","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":[]},"/about-me":{"title":"About me üôã‚Äç‚ôÇÔ∏è","content":"\nMy name is Simon LEONARD. This project has been produced during my 4th year after secondary school/_Lyc√©e_. \n\nWhy am I not writing BSc Hons? And why do I use the French word _Lyc√©e_?\n\nBecause I'm currently a French student, and I'm in a Erasmus+ exchange year in [Dundalk Institute of Technologies](https://www.dkit.ie/) at the time of writing.\\\nAt DkIT I'm following the course _Computing in Cloud Computing_, meanwhile at [Universit√© Savoie Mont-Blanc](https://univ-smb.fr/en) I'm enrolled in the [CMI programme](https://reseau-figure.fr/about-cmi/?lang=en) for IT, which is basically a bundle of BSc + BSc (hons) + MSc + additional courses on 5 years.\n\nSee my [Linkedin](https://linkedin.com/in/simon-l5d) and [Github](https://github.com/sinux-l5d).","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":[]},"/journal/2023-02-10":{"title":"2023-02-10","content":"\nToday I start experimenting with the Podman API through its library in Go. I'm not trying Docker API, because I know I want rootless for security measures (thought it's doable, but wasn't so easy last time I checked). I'll have to dig into https://docs.docker.com/engine/security/rootless/ to be sure.\n\nThe first issue when trying Podman for the first time is that it requires to have multiple uids/gids set for a user in order to use the [rootless mode](https://docs.podman.io/en/latest/markdown/podman.1.html#rootless-mode).\nSo:\n```bash\nsudo usermod --add-subuids 10000-75535 $(whoami)\nsudo usermod --add-subgids 10000-75535 $(whoami)\n```\n\nNow, `podman images list` should run.\n\n\u003e [!info] \n\u003e\n\u003e Note, that the above range for the user¬†`_username_`¬†may already be taken by another user as it defines the default range for the first user on the system. If in doubt, first consult the¬†`/etc/subuid`¬†and¬†`/etc/subgid`¬†files to find the already reserved ranges.\n","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-02-12":{"title":"2023-02-12","content":"\nStill on Podman API.\nPodman don't work like docker: the command line directly take care of the operations, whereas the in docker the command line \"translate\" the command in a API call to the Docker Engine through unix socket.\n\nhttps://www.reddit.com/r/podman/comments/110j7i3/running_containers_from_go_program_rest_api_or_lib/\n\nIn order to spawn containers from the host, I can use either the lib of podman (written in go) or start the daemon REST API and use the bindings.\nWhy would I use the later option, as it's basically the same as docker? Because the daemon starts with the user's privilege, so in rootless by default.\nBut if my program is in a container, it can only use the REST API. And so, I need to mount the socket file as a volume.\n\nLet's say I'm using the second approach (simpler for admin). An blog post was written in 2020 about this: https://podman.io/blogs/2020/08/10/podman-go-bindings.html.\n\nAlso to read:\n- https://blog.podman.io/2023/02/the-container-name-resolution-conundrum/\n- https://www.redhat.com/sysadmin/controlling-access-rootless-podman-users\n- https://www.redhat.com/sysadmin/behind-scenes-podman\n\nhttps://wiki.archlinux.org/title/Podman says that podman require fuse-overlayfs to run in rootless.\n\nPodman doesn't have any registry by default, so `podman pull httpd` wouldn't work. It's the job of the admin to first pull my container, but **the Manager should use a absolute path to pull and run containers**.\nfrom /etc/containers/registries.conf:\n\n\u003e We recommend always using fully qualified image names including the registry server (full dns name), namespace, image name, and tag (e.g., registry.redhat.io/ubi8/ubi:latest) [...]\n\u003e When using short names, there is always an inherent risk that the image being pulled could be spoofed.\n\n\u003e [!note]\n\u003e \n\u003e When trying to pull the hello-world image, I discovered that docker's container like https://hub.docker.com/_/hello-world have the full name docker.io/library/hello-world\n\nI've been using the v2 lib of podman for a few hours before noticing latest is v4...","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-02-14":{"title":"2023-02-14","content":"\nStarted coding. I have a library that list containers owned by my app (using containers labels) and run a new one (dummy, busybox for now).\nThe client is for now a basic CLI for development.\n\nI'm trying to compile with `CGO_ENABLED=0`, but it don't work.\n\n\u003e [!info]\n\u003e\n\u003e CGO_ENABLED is a environment variable for the go compiler that allow the use of external C code, for compatibility. My project indirectly rely on github.com/containers/image which require CGO for signing.\n\nMoreover, from [Podman's Makefile](https://github.com/containers/podman/blob/5865159766d56229899432cb1a72949d171e860c/Makefile#L161):\n\u003e Podman does not work w/o CGO_ENABLED, except in some very specific cases.\n\nSo I will keep the CGO, but that means shared library (.so) files has to be present","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-02-16":{"title":"2023-02-16","content":"\nBecause I can't make a statically linked binary, I will stick with an alpine container for now. Post on stackoverflow: https://stackoverflow.com/questions/75459376/statically-link-devmapper-when-using-cgo\n\nAbout ENTRYPOINT vs CMD in a Dockerfile/Containerfile:\n\u003e The _shell_ form prevents any `CMD` or `run` command line arguments from being used, but has the disadvantage that your `ENTRYPOINT` will be started as a subcommand of `/bin/sh -c`, which does not pass signals. This means that the executable will not be the container‚Äôs `PID 1` - and will _not_ receive Unix signals - so your executable will not receive a `SIGTERM` from `docker stop \u003ccontainer\u003e`.\n\u003e -\u003e *https://docs.docker.com/engine/reference/builder/#entrypoint*\n\n\nI'm starting to write the basic for spawning a container, listing them etc. I use labels to store a container info.\n\nI'm also getting to know podman's pod, which I think would become handy for grouping containers of only one project (e.g. apache + php + mysql): https://www.cloudreach.com/en/technical-blog/containerize-this-how-to-use-php-apache-mysql-within-docker-containers/.\nI originally wanted to have one container per project, but that require to build a containerfile that launch (in this case) 3 containers. It doesn't scale well, takes time, and the commendation for containers is to have a single process as PID 1, to handle graceful shutdown. ","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-02-17":{"title":"2023-02-17","content":"\nI had a problem this morning: each container will have a different name, and therefore it won't be possible for a proxy container to know at build time other containers name. So I could not use `ProxyPassMatch ... fastcgi://php-fpm:9000` in my Apache config (php-fpm would be resolve as the real container ip address)\n\nHowever, within a pod it seams like every container has the same localhost, and therefor I can use something like `ProxyPassMatch fastcgo://127.0.0.1:9000`. I discovered that by creating a pod and running a container inside, then `cat /etc/host` which revealed that the containers' hostname were mapped to 127.0.0.1.\n\n\u003e [!example]\n\u003e\n\u003e To demonstrate this, first create a pod:\n\u003e\n\u003e ```bash\n\u003e podman pod create --name nc-test\n\u003e ```\n\u003e\n\u003e Then run a container running netcat in listening mode, and another that will send a message using the hostname of the listener:\n\u003e\n\u003e ```bash\n\u003e podman run --rm --pod nc-test --name alp1 docker.io/library/alpine:latest nc -l -p 2589\n\u003e podman run --rm --pod nc-test --name alp2 docker.io/library/alpin  \ne:latest sh -c 'echo \"hello world\" | nc alp1 2589'\n\u003e ```\n\u003e\n\u003e The listener show `hello world`. Now let's do the same, but this time the sender will use 127.0.0.1:\n\u003e\n\u003e ```bash\n\u003e podman run --rm --pod nc-test --name alp1 docker.io/library/alpin  \ne:latest nc -l -p 2589\n\u003e podman run --rm --pod nc-test --name alp2 docker.io/library/alpin  \ne:latest sh -c 'echo \"hello world\" | nc 127.0.0.1 2589'\n\u003e ```\n\u003e\n\u003e It shows `hello world` too!","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-02-21":{"title":"2023-02-21","content":"\nEnhancing the CLI by importing new module https://github.com/urfave/cli\n","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-02-23":{"title":"2023-02-23","content":"\nI just made progress: Apache and PHP container talked to each other inside of a pod.\nAlso, I statically compiled my app thanks to https://github.com/containers/podman/issues/12548#issuecomment-989053364.\nThe Podman bindings actually don't require a lot of things from its libraries, so adding tags when compiling (like `remote` and others) remove the need for any external C dependencies, and so I can deactivate CGO, and build my final stage from `scratch`.\n\nMy final container is 35 MB. I reduced it to 15.5 MB with `upx --lzma`.\n\nI'm now writing a script for conventional commits.\nhttps://gist.github.com/sinux-l5d/e80b4d1ba252c53b1f967b4ce7d18d17\n","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-02-26":{"title":"2023-02-26","content":"\nAdd first \"runtime\", which is a set of container to run a certain type of project. First one is a LAMP (apache php mysql).\n\nI'm also working on how to build containers with github workflow.\nThis is a challenge because usually 1 repo = 1 container image (with variants based on label). \nI don't want to build every runtime each time a new version is released, so I check for files changed since previous tag. If something has changed in `containerfiles/`, build only the runtime which has changed. \nAlso challenging because no for loop exist, I have to use `strategy.matrix`.\nFinally, I haven't checked yet if ghcr.io allow to used non-repository based packages. My repo is as github.com/sinux-l5d/studentbox, which means I can have a package at ghcr.io/sinux-ld5/studentbox, but I don't know if I can create a package ghcr.io/sinux-ld5/studentbox-lamp-apache and so on. If not, I will have to either use labels (improper use) or use external registry (quay.io or docker.io)","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-02-27":{"title":"2023-02-27","content":"\nI've managed to set up a workflow to build runtime pods (so a set of containerfile) in a test repository. It build and publish to the ghcr.io registry.\n\nI managed to have my runtime containers building, and being published to ghcr.io on each new tag (so a version).\n\nI'm now encountering an issue: when a container is runned, its file might not belong to the host's user, resulting in file I can't read/write/delete.\nI should read https://www.redhat.com/en/blog/understanding-root-inside-and-outside-container tomorrow for further details.\nAlso: https://podman.io/blogs/2018/10/03/podman-remove-content-homedir.html","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-02-28":{"title":"2023-02-28","content":"\nRDV Stephen:\nfinal documentation: write the different steps I've been through (use journal) (eg lib, then cli, then server, then client)","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-03-02":{"title":"2023-03-02","content":"\nToday I'm working on automating binary creation and container binary with a Makefile and workflow, and embed version number in go binary (with `go build -ldflags ...`)","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-03-25":{"title":"2023-03-25","content":"\nI'm continuing the ethical report started earlier this week.","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-03-28":{"title":"2023-03-28","content":"\nAfter reading [this article from go.dev](https://go.dev/blog/context-and-structs), I know I should extract the context of my internal lib from the Mangager struct. However, this would require deep changes, and the need to merge the library's context (from podman) with the user's one. I posted on Reddit: https://www.reddit.com/r/golang/comments/124qrin/handling_two_contexts/","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-04-21":{"title":"2023-04-21","content":"\nToday I'm refactoring to use pods directly. It's complicated because the documentation is not self-explanatory, so I often have to dig into [podman's cli code](https://github.com/containers/podman).","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-04-22":{"title":"2023-04-22","content":"\nContinuing yesterday's work of refactoring to use pods. It's been tough to find how to store the configuration for pods, as some config can be per image or shared between images. I've used a combination of labels, and scripts to generate a list of available config (internal/runtimes) with go generate.\nNow I should be able to finish the pod spawning.\n\nI can now spawn Busybox with a specially crafted runtimes.Runtime.\n\nNow I have an issue with a real example (lamp runtime) when pulling an image: I get a 403 error from ghcr.io. I think it's because my computer's config for /etc/containers/registries.conf don't include ghcr.io.\nWrong lead: just a bad URL... Working now.\n\nI must have failed something, my data directory looks like this:\n```\ndata/\n‚îú‚îÄ‚îÄ sb-sinux-php-test-apache/\n‚îÇ  ‚îî‚îÄ‚îÄ html/\n‚îú‚îÄ‚îÄ sb-sinux-php-test-mysql/\n‚îÇ  ‚îî‚îÄ‚îÄ db/\n‚îî‚îÄ‚îÄ sb-sinux-php-test-php/\n   ‚îî‚îÄ‚îÄ html/\n```\n\ninstead of this:\n```\ndata/\n‚îú‚îÄ‚îÄ sb-sinux-php-test\n‚îÇ  ‚îú‚îÄ‚îÄ html/\n‚îÇ  ‚îî‚îÄ‚îÄ db/\n```\n\nThat's actually because I create directories by container name, not project.\nBut let's call it a day, it spawns my pod:\n```\n‚ûú podman ps -a --pod\nCONTAINER ID  IMAGE                                                    COMMAND           CREATED        STATUS                    PORTS       NAMES                     POD ID        PODNAME\n69160de186ff  localhost/podman-pause:4.5.0-1681856273                                    4 minutes ago  Up 4 minutes                          94a6f20e6260-infra        94a6f20e6260  sb-sinux-php-test\n9e5d6767dcf6  ghcr.io/sinux-l5d/studentbox/runtime/lamp.apache:latest  httpd-foreground  4 minutes ago  Up 4 minutes                          sb-sinux-php-test-apache  94a6f20e6260  sb-sinux-php-test\nce7dc3b2157a  ghcr.io/sinux-l5d/studentbox/runtime/lamp.mysql:latest   mariadbd          4 minutes ago  Exited (1) 4 minutes ago              sb-sinux-php-test-mysql   94a6f20e6260  sb-sinux-php-test\n24d716a69dd3  ghcr.io/sinux-l5d/studentbox/runtime/lamp.php:latest     php-fpm           4 minutes ago  Up 4 minutes                          sb-sinux-php-test-php     94a6f20e6260  sb-sinux-php-test\n```","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-04-24":{"title":"2023-04-24","content":"\nFixed issue from [[journal/2023-04-22|2023-04-22]] with created directories for container. Now it looks like this:\n```\ndata\n‚îî‚îÄ‚îÄ user\n    ‚îî‚îÄ‚îÄ test-lamp\n        ‚îú‚îÄ‚îÄ db\n        ‚îî‚îÄ‚îÄ html\n```\n\nI noticed my MySQL container exited with status `1`. With a quick `podman logs`, I figured out I omitted environment variables.\nBut quickly, I had a problem: some variable have default values, others has to be generated later (example, password), others must be provided and fail if not. I've created a `envvar.go` file in `internal/runtimes` to address this issue.\n\nAt the core, env var have a name, a default values, and what I called modifiers. Just as for mount points, I parse Dockerfile to retrieve the information and put it in a struct. Default values come from `ENV` instructions, modifiers and var without default come from label `studentbox.config.env`. This is to assure compatibility with any tool using the `ENV` instructions of dockerfiles/containerfile. But that might change later, so I keep the coupling level at the minimum. Note from Michelle: it's anticipating change. Meaning parsing and applying modifiers is two distinct jobs (both deal with `EnvVar` struct).\n\nIn the end, I declare \"official\" runtimes with `go generate`, that will read and parse files in `./runtimes/` to create file `./internal/runtimes/generated.go`, that export a single map `OfficialRuntimes`. A new runtime just have to create a new directory in `./runtimes`, create the needed containerfile, add labels and that's it: runtime will be build by Github actions and will have an official entry in OfficialRuntimes.\n\nI've written unit tests for `envvar.go`.","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/journal/2023-05-06":{"title":"2023-05-06","content":"\nToday I'm implementing the EnvVar modifiers I've been writing last time.\nIt works, my MariaDB container now has a default user, default database, and generate a password (`:password` modifier) for both root and user.\nFor now, the environment variables submitted by the user are written _in all containers_. That require some more CLI design to bound it to a container only, more time than I have.\n\nBut now that my runtime is up, I realised I've forgot to expose a port in container. Yet another problem...\nFortunately, I already have a EXPOSE line in a containerfile, and Podman can find a free port by itself, so it should be fairly easy.\n\nI think I won't have time to implement the web-server. Instead, a teacher would have to use the CLI from the server that host containers to create student and projects. The teacher would then need to setup a SFTP server for the student to upload his code.","lastmodified":"2023-05-06T19:43:30.166064253Z","tags":["journal"]},"/project/goals":{"title":"Goals üéØ","content":"\n# Back end\n\nThe back end should be either hosted on AWS or an on-premise server.\n\nAmazon's cloud would be useful if one needs to easily scale as more students use it.\nOn another hand, an on-premise server allow flexibility and complete control other data, privacy and hosting.\n\nThis means that I cannot use AWS's specific service, or at least not without increasing development cost.\n\nFinally, an easy/automated installation would save time.\n\n## Isolate process\n\nWe don't want that students' project impact other's by accident or on purpose.\n\nIf a web app crash, is heavily dealing with I/O, or is stuck in a infinite loop, it should impact as little as possible other projects.\n\n## Data exchange\n\nThe API should be bidirectional, for let's say uploading files and getting live logs of the back end.\n\n## Live reload\n\nLive reload is a convenient feature to allow to see changes as soon as a file is saved. In our case, that would mean upload the changed files and eventually restart the webapp.  \n\n## Users \u0026 permissions\n\nStudent and teachers are both users. Teachers can host his own project, for demonstration purpose let's say. He can invite students to register in his class, and manage the projects associated.\n\n## Git integration üí°\n\nUse Git as part of the API, as a backup/rollback system.\nEach project would have a repository.\n\n# CLI\n\nA CLI client for all users. A student should be able to deploy his project, retrieve logs (possibly in live), get link. A teacher has the same rights as student, plus managing resources, users, projects.\n\nBecause most users use Windows, the CLI needs to be cross platform.\n\n# Front end\n\nSimple webapp that allow the same thing as CLI, in a more graphical manner. No install, drag and drop zip/archive to deploy. \nNote for the ZIP: mind that code might be at root of the archive, not in a top-level directory.\n\nIt should present live logs, just like CLI.\n","lastmodified":"2023-05-06T19:43:30.1700643Z","tags":["project"]},"/project/road-map":{"title":"Road map üó∫Ô∏è","content":"\n\n# Research üîé\n\nThis section treats about everything that needs to be researched before starting to code.\n\n## API \n\nCompare API types, knowing I want to transfer files, and need live-capabilities.\n\n### JSON\n\nSee [[research/api/json|JSON]]\n\n\u003c!-- ![JSON](research/api/json) --\u003e\n\n### REST \n\nSee [[research/api/rest|REST]].\n\n\n### GraphQL\n\nSee [[research/api/graphql|GraphQL]].\n\n### gRPC \u0026 Protobuf\n\nSee [[research/api/protobuf|Protobuf]] and [[research/api/grpc|gRPC]].\n\n## Sand-boxing processes \n\nIt's important that each project is isolated from others and eventually from the host. I need a lightweight system that allow to easily add/remove new project, limiting disk/CPU/memory usage.\n\n### Container approach\n\n#### Docker\n\n#### Podman\n\n#### Kubernetes\n\nSee implementations and API\n\n### Virtual Machines approach\n\n#### Vagrant\n\n#### Self-managed\n\n#### AWS-managed\n\n## Programming language \n\n## Storage\n\nThe app will 2 different types of data: metadata (users, permissions) and the projects themselves (files).\nWhat type of database? Why?\n\n### Users, permissions, projects...\n\n### Projects\n\n## Logic\n\n### Defining permissions\n\n### Recipe for minimal deploy\n\nAll student in mind, write down the simplest way for a user to deploy his php/node.js app.\n\n# Choosing tech stack (by 16th of January)\n\nOnce research is done, make choices.\n\n# Realising","lastmodified":"2023-05-06T19:43:30.1700643Z","tags":["project"]},"/research/api/graphql":{"title":"GraphQL","content":"\n\u003e [!quote] According to the official website\n\u003e\n\u003e GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer¬†tools.\n\u003e\n\u003e \u003csmall\u003e_GraphQL | A query language for you API_ (no date) _GraphQL_. Available at: https://graphql.org/ (Accessed: January 13, 2023).\u003c/small\u003e\n\nFirst thing we notice is that GraphQL is not just a API specifications but a full runtime[^aimuliple] unlike [[research/api/rest|REST]] which is just a specification.\n\nFor GraphQL, just as for [[research/api/grpc|gRPC]], we need to write specification of the objects we will transfer beforehand. In the GraphQL word, it's called _type definitions_ and types contains _fields_.\n\n\u003e [!example] Example from the official website\n\u003e\n\u003e First describe the objects\n\u003e\n\u003e ```graphql\n\u003e type Project {\n\u003e   name: String\n\u003e   tagline: String\n\u003e   contributors: [User]\n\u003e }\n\u003e ```\n\u003e\n\u003e Then query whatever you want\n\u003e\n\u003e ```graphql\n\u003e {\n\u003e   project(name: \"GraphQL\") {\n\u003e     tagline\n\u003e   }\n\u003e }\n\u003e ```\n\u003e\n\u003e The result will be formatted just like the query\n\u003e\n\u003e ```graphql\n\u003e {\n\u003e   \"project\": {\n\u003e     \"tagline\": \"A query language for APIs\"\n\u003e   }\n\u003e }\n\u003e ```\n\nThose type definition being in separate files, it's easy to document the API automatically with tools like [Graph*i*QL](https://github.com/graphql/graphiql).\n\nOne of the advantages of this query approach is its precision, allowing us to have all the data we need at once, without the need to fetch multiple times like it's the case for REST.\n\n\u003e [!danger] Drawbacks\n\u003e\n\u003e From https://research.aimultiple.com/graphql-vs-rest/\n\u003e\n\u003e 1. GraphQL's single endpoint (`/graphql`) can be a bottleneck, especially because REST allows easy caching on routes.\n\u003e 2. Security is not built in the standard, different alternatives exist.\n\u003e 3. \"While GraphQL offers a caching mechanism, the cache-implementing process is much more complex and time-consuming than REST implementation.\" It's mostly because REST allows caching by using different URLs.\n\u003e 4. GraphQL costs more because queries can be unpredictably large, and a query's cost can be hard to estimate.\n\n## File upload\n\nJust like for REST, we could encode files in base64 to upload them. But that adds overhead for both server and client, increasing the file size by ~33%[^wundergraph].\n\nJust like for REST, we could use a multipart request but:\n\n\u003e [!quote] Quote from Jens Neuse[^wundergraph]\n\u003e\n\u003e Unfortunately, there's no standard to handle Multipart Requests with GraphQL.¬†This means, your solution will not be easily portable across different languages or implementations¬†and your client implementation depends on the exact implementation of the server.\n\nSo one of the other approaches is to have another homemade API in REST that takes care of files only, or using AWS S3. The last option is not viable if we want to keep our system independent from the cloud.\n\nThe article I relied on is made by Jens Neuse at [WunderGraph](https://wundergraph.com), described as \"The¬†simplicity of RPC with the¬†power of GraphQL\" on their website. That could be a solution, but would lock us in a not-so-well-known solution, and we don't control the prices. If I where to use RPC, I would probably go with [[research/api/grpc|gRPC]] which is free, open source and licensed Apache-2.0.\n\n[^aimuliple]: Ataman, A. (2023) _Graphql vs. rest in 2023: Top 4 advantages \u0026 disadvantages_, _AIMultiple_. Available at: https://research.aimultiple.com/graphql-vs-rest/ (Accessed: January 13, 2023).\n[^wundergraph]: Neuse, J. (2021) _GraphQL file uploads - evaluating the 5 most common approaches_, _WunderGraph_. Available at: https://wundergraph.com/blog/graphql_file_uploads_evaluating_the_5_most_common_approaches (Accessed: January 13, 2023).\n","lastmodified":"2023-05-06T19:43:30.1700643Z","tags":[]},"/research/api/grpc":{"title":"gRPC","content":"\ngRPC is \"a high performance, open source universal RPC framework\"[^grpcio].\nRPC stands for _Remote Procedure Call_, which is a kind of API that abstracts the complexity of making calls to a remote API by providing to the developer functions that works like if it was local[^wikipedia].\n\n\u003e [!quote] Quote\n\u003e\n\u003e gRPC uses HTTP/2 under the covers, but HTTP is not exposed to the API designer[^googlecloud]\n\ngRPC uses [[research/api/protobuf|Protocol Buffers]] as a serialisation format by default, but can use [[research/api/json|JSON]]. It works by extending the syntax of a `.proto` file.\n\nWe need to define the messages (basic Protobuf structure) that will be exchanged whenever it's an argument or a return value, along with the `services`, that describe the procedures available.\n\n\u003e [!example] Example from official tutorial[^grpcio]\n\u003e\n\u003e First the messages:\n\u003e\n\u003e ```protobuf\n\u003e // The request message containing the user's name.\n\u003e message HelloRequest {\n\u003e   string name = 1;\n\u003e }\n\u003e\n\u003e // The response message containing the greetings\n\u003e message HelloReply {\n\u003e   string message = 1;\n\u003e }\n\u003e ```\n\u003e\n\u003e and then the gRPC syntax to declare a _service, a set of procedures_:\n\u003e\n\u003e ```protobuf\n\u003e // The greeter service definition.\n\u003e service Greeter {\n\u003e   // Sends a greeting\n\u003e   rpc SayHello (HelloRequest) returns (HelloReply) {}\n\u003e }\n\u003e ```\n\nA few advantages of gRPC:\n\n1. It abstracts away the calls to a remote API\n2. Compatible with several popular languages\n3. It makes possible to work with native objects of your current programming language\n4. It generate the classes and object/messages for you from a `.proto` file\n\nFrom the dedicated file, it creates a gRPC server to receive requests from clients that themselves use a dynamic library generated via `protoc` and a dedicated plugin.\n\n![Architecture of gRPC](research/api/grpc-architecture.svg)\n\nThis diagram illustrate how different clients/servers written in different languages can work together with the gRPC server and _stubs_ (a client) created.\n\n## File upload\n\nAlthough a few users would recommend to use a [[research/api/rest|REST]] API to handle uploads, other says that it depends on the size of images and your app[^reddit_grpc_files].\n\nImplementing file uploading in gRPC seems to be done via `stream`s[^betterprogramming][^vinsguru], one of [its features](https://grpc.io/docs/what-is-grpc/core-concepts/#client-streaming-rpc). The core principle is to send files in small chunk (by default, gRPC limits incoming messages to¬†*4 MB* in case the developer hasn't thought about message size, but it can be increased[^sof_4mb]) Note that we send the metadata apart from the file, and it's not a standard so we have to define what's inside (path, name, extension/type...).\n\nImplementations seem to also use `oneof` keyword to avoid sending metadata each time we're sending a chunk of file.\n\n```protobuf\nmessage MetaData {\n\tstring filename = 1;\n\tstring extension = 2;\n}\n\nmessage FileUploadRequest {\n\toneof request {\n\t\tMetaData metadata = 1;\n\t\tbytes chunk = 2;\n\t}\n}\n```\n\n[^grpcio]: _Introduction to grpc_ (2022) _gRPC_. Available at: https://grpc.io/docs/what-is-grpc/introduction/ (Accessed: January 14, 2023).\n[^wikipedia]: _Remote procedure call_ (2023) _Wikipedia_. Wikimedia Foundation. Available at: https://en.wikipedia.org/wiki/Remote_procedure_call (Accessed: January 14, 2023).\n[^googlecloud]: Nally, M. (2020) _GRPC vs rest: Understanding grpc, openapi and rest and when to use them in API design | google cloud blog_, _Google_. Google. Available at: https://cloud.google.com/blog/products/api-management/understanding-grpc-openapi-and-rest-and-when-to-use-them (Accessed: January 14, 2023).\n[^reddit_grpc_files]: _r/grpc - grpc is suitable to handle file uploads?_ (no date) _reddit_. Available at: https://www.reddit.com/r/grpc/comments/oj2k9n/grpc_is_suitable_to_handle_file_uploads/ (Accessed: January 14, 2023).\n[^betterprogramming]: Foong, N.W. (2022) _GRPC file upload and download in Python_, _Medium_. Better Programming. Available at: https://betterprogramming.pub/grpc-file-upload-and-download-in-python-910cc645bcf0 (Accessed: January 14, 2023).\n[^vinsguru]: vIns (2022) _GRPC file upload with client streaming_, _Vinsguru_. Available at: https://www.vinsguru.com/grpc-file-upload-client-streaming/ (Accessed: January 14, 2023).\n[^sof_4mb]: Anderson, E. (2019) _Should I transmit large data sets via grpc without manual chunking?_, _Stack Overflow_. Available at: https://stackoverflow.com/a/58435745 (Accessed: January 14, 2023).\n","lastmodified":"2023-05-06T19:43:30.1700643Z","tags":[]},"/research/api/json":{"title":"JSON","content":"\n\u003e [!quote] From the official website\n\u003e\n\u003e **JSON**¬†(JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate.\n\u003e\n\u003e \u003csmall\u003e_Introducing json_ (no date) _JSON_. Available at: https://www.json.org/json-en.html (Accessed: January 13, 2023).\u003c/small\u003e\n\nIt's a serialisation format that is human-friendly, meaning objects, arrays and values are represented in a string. Example:\n\n```json\n{\n  \"name\": \"Simon\",\n  \"surname\": \"Leonard\",\n  \"age\": 21,\n  \"hobbies\": [\"cooking\", \"reading\", \"drinking coffee\"],\n  \"enemies\": null\n}\n```\n\nHere is a speculative object that could represent me in JSON.\n\nThis time, let's say I have a array of users:\n\n```json\n{\n  \"users\": [\n    {\n      \"id\": \"12345\",\n      \"name\": \"Simon\",\n      \"surname\": \"Leonard\",\n      \"age\": 21,\n      \"hobbies\": [\"cooking\", \"reading\", \"drinking coffee\"],\n      \"allies\": [\"54321\"]\n    },\n    {\n      \"id\": \"54321\",\n      \"name\": \"Lucas\",\n      \"surname\": \"Thomas\",\n      \"age\": 22,\n      \"hobbies\": [],\n      \"allies\": [\"12345\"]\n    }\n  ]\n}\n```\n\nWe can see that JSON has at least one drawback: the document's structure and the data are mixed. You can see above that we repeat the keys `id`, `name`, `surname`... for multiple users.\n\n\u003e [!question] Is it possible to transfer a file over JSON?\n\u003e\n\u003e Although protocols like REST and gRPC are more suitable for the task (with multipart upload and stream respectively), one could upload a file by encoding it in base64. Note that it increase the data size by ~33%[^33-percent]\n\n[^33-percent]: T, D. and B, S. (2019) _Posting a file and associated data to a restful webservice preferably as JSON_, _Stack Overflow_. Available at: https://stackoverflow.com/a/4083908 (Accessed: January 5, 2023).\n","lastmodified":"2023-05-06T19:43:30.1700643Z","tags":[]},"/research/api/protobuf":{"title":"Protocol Buffers","content":"\n[Protobuf or Protocol Buffers](https://developers.google.com/protocol-buffers) is a binary serialisation format that needs type definition beforehand. It's created by Google. This allows to omit structure from the data serialised, as client and server know what it is about.\n\n\u003e [!quote]\n\u003e\n\u003e Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured data.\n\u003e \u003csmall\u003e_Protocol buffers | google developers_ (no date) _Google_. Google. Available at: https://developers.google.com/protocol-buffers (Accessed: January 13, 2023).\u003c/small\u003e\n\nTo demonstrate how it's working, I've recreated the same structure as in the [[research/api/json|JSON]] page.\n\nA _type_ in protobuf is called a `message`. Messages are defined in a `.proto` file. Let's create a list of people, and see how it's serialised compared to JSON.\n\n```protobuf\nsyntax = \"proto3\";\npackage file;\n\nmessage Person {\n    string id = 1;\n    string name = 2;\n    string surname = 3;\n    uint32 age = 4;\n    repeated string hobbies = 5;\n    repeated string allies = 6;\n}\n\nmessage PersonList {\n    repeated Person users = 1;\n}\n```\n\nBefore going further, each property has an identifier (the numbers). They are used in serialisation.\n\nI made a little experiment in JavaScript using [protobufjs](https://www.npmjs.com/package/protobufjs) to compare the length of messages in JSON (string) and Protobuf (binary):\n\n```js\nconst protobuf = require(\"protobufjs\")\n\nconst simon = {\n  id: \"12345\",\n  name: \"Simon\",\n  surname: \"Leonard\",\n  age: 21,\n  hobbies: [\"coding\", \"reading\", \"writing\"],\n  allies: [\"54321\"],\n}\n\nconst lucas = {\n  id: \"54321\",\n  name: \"Lucas\",\n  surname: \"Thomas\",\n  age: 22,\n  hobbies: [],\n  allies: [\"12345\"],\n}\n\nconst root = protobuf.loadSync(\"file.proto\")\n\nconst Person = root.lookupType(\"file.Person\")\n\nvar simonBuf = Person.create(simon)\nvar lucasBuf = Person.create(lucas)\n\nconst PersonList = root.lookupType(\"file.PersonList\")\nconst users = PersonList.create({ users: [simonBuf, lucasBuf] })\nconst dataUsers = PersonList.encode(users).finish()\n\nconsole.log(\"proto-serialized=\" + dataUsers.length)\nconsole.log(\"json-serialized=\" + JSON.stringify(users).length)\n```\n\nThe score for proto is 93 bytes, and 207 for JSON.\n\n\u003e [!info] Info\n\u003e\n\u003e When a protobufjs object like Person or Personlist gets serialised as JSON, like the last line in the script above, then it uses the JavaScript equivalent, so the result is not biased.\n\nWith a few more line, we can infer what the proto-serialised message look like:\n\n```js\nconsole.log(dataUsers) // \u003cBuffer 0a 3a 0a 05 31 32 33 34 35 12 05 53 69 6d 6f 6e 1a 07 4c 65 6f 6e 61 72 64 20 15 2a 06 63 6f 64 69 6e 67 2a 07 72 65 61 64 69 6e 67 2a 07 77 72 69 74 ... 43 more bytes\u003e\nconsole.log(dataUser.toString())\n//\n// :\n// 12345SimonLeonard *coding*reading*writing254321\n//\n// 54321LucasThomas 212345\n```\n","lastmodified":"2023-05-06T19:43:30.174064347Z","tags":[]},"/research/api/rest":{"title":"REST","content":"\nREST (_Representational state transfer_) is a style of software architecture that takes advantage of HTTP(S):\n\n- The path represent a resource\n- A verb represent what kind of operation has to be done\n- Headers are used for various things, like authorisation token\n- The body for the actual data, when needed, nowadays mostly [[research/api/json]] for APIs\n- The return status tells quickly if everything happened as expected\n\nLet's say we have a database of recipes. We want to list them, add, modify and delete them. This can be done with a meaningful base path like `/recipes`. An API following REST principles would have those couples verbs/paths:\n\n- `GET /recipes` to have a list of them (complete or partial objects, maybe with pagination)\n- `POST /recipes` to add one\n- `PUT /recipes/:id` or `PATCH /recipes/:id` to modify a recipe fully or partially (where id is a unique identifier of the resource)\n- `DELETE /recipes/:id` to delete it\n\n## File upload\n\nFor uploading files, we have several options. One is to use base64 encoding and put the result in JSON, but it's not the best option for large files. Although we might never have big files (because we exchange coding files), it is something to consider for assets for example.\n\nAnother way of doing this through a `multipart/form-data` request[^multipart]. The principle is to first upload the files independently from their metadata or any data you could send. Either first upload the files, get IDs and link them to the metadata (server controls names/ids) or upload metadata first and then files (client controls names/ids).\n\n[^multipart]: libik and kirk (2020) _REST API - file (ie images) processing - best practices_, _Stack Overflow_. Available at: https://stackoverflow.com/questions/33279153/rest-api-file-ie-images-processing-best-practices (Accessed: January 13, 2023).\n","lastmodified":"2023-05-06T19:43:30.174064347Z","tags":[]}}